{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWxb2U0Byi4n"
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip -q --show-progress\n",
        "!wget https://s3-us-west-2.amazonaws.com/dl.fbaipublicfiles.com/LVIS/lvis_v1_train.json.zip -q --show-progress\n",
        "\n",
        "!unzip -qq \"./train2017.zip\" -d \"./train\"\n",
        "!unzip -qq \"./lvis_v1_train.json.zip\" -d \"./labels\"\n",
        "\n",
        "!pip install -q lvis\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\n",
        "!git clone https://github.com/ultralytics/yolov5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5K6pgKc2Iin",
        "outputId": "0da4ae3b-a13b-44da-a9b9-3939465d7f8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "from shutil import copyfile\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "\n",
        "from lvis import LVIS\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.device(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFVEjmUe3Kl9"
      },
      "outputs": [],
      "source": [
        "lvis_api = LVIS(\n",
        "    \"labels/lvis_v1_train.json\"\n",
        ")  # –∫–æ–ª–∞–± –∫—Ä–∞—à–∏—Ç—Å—è –µ—Å–ª–∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–¥–µ–ª–∞—Ç—å json.load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tljX71WP9_0"
      },
      "outputs": [],
      "source": [
        "def get_helmet_dataset_dicts(lvis_api):\n",
        "    img_ids = sorted(lvis_api.imgs.keys())\n",
        "    imgs = lvis_api.load_imgs(img_ids)\n",
        "    anns = [lvis_api.img_ann_map[img_id] for img_id in img_ids]\n",
        "    \n",
        "    def get_file_name(img_root, img_dict):\n",
        "        split_folder, file_name = img_dict[\"coco_url\"].split(\"/\")[-2:]\n",
        "        return os.path.join(img_root + split_folder, file_name)\n",
        "\n",
        "    dataset_dicts = []\n",
        "\n",
        "    for (img_dict, anno_dict_list) in zip(imgs, anns):\n",
        "        record = {}\n",
        "        record[\"file_name\"] = get_file_name(\"train/\", img_dict)\n",
        "        record[\"height\"] = img_dict[\"height\"]\n",
        "        record[\"width\"] = img_dict[\"width\"]\n",
        "        record[\"image_id\"] = img_dict[\"id\"]\n",
        "        objs = []\n",
        "        has_helmet = False\n",
        "        for anno in anno_dict_list:\n",
        "            # helmet & football_helmet, –±–æ–ª—å—à–µ –Ω–∏—á–µ–≥–æ —Å –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π helmet –Ω–µ—Ç\n",
        "            if anno[\"category_id\"] == 556 or anno[\"category_id\"] == 467:\n",
        "                objs.append(anno[\"bbox\"])\n",
        "                has_helmet = True\n",
        "        record[\"bbox\"] = objs\n",
        "        if has_helmet:\n",
        "            dataset_dicts.append(record)\n",
        "            has_helmet = False\n",
        "    return dataset_dicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJwQrUGhP_fk"
      },
      "outputs": [],
      "source": [
        "dataset_dicts = get_helmet_dataset_dicts(lvis_api)\n",
        "train_dataset_dicts, val_dataset_dicts = train_test_split(dataset_dicts, test_size=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK0oBVfvTF5N",
        "outputId": "57d68246-fa24-4f78-e7b6-df21a6bb363e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1718/1718 [00:06<00:00, 252.07it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:00<00:00, 232.43it/s]\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"./lvis/images/train\", exist_ok=True)\n",
        "os.makedirs(\"./lvis/images/valid\", exist_ok=True)\n",
        "\n",
        "os.makedirs(\"./lvis/labels/train\", exist_ok=True)\n",
        "os.makedirs(\"./lvis/labels/valid\", exist_ok=True)\n",
        "\n",
        "for img_data in tqdm(train_dataset_dicts):\n",
        "    img_path = img_data[\"file_name\"]\n",
        "    img_name = img_path.split(\"/\")[-1]\n",
        "    copyfile(f\"{img_path}\", f\"./lvis/images/train/{img_name}\")\n",
        "\n",
        "for img_data in tqdm(val_dataset_dicts):\n",
        "    img_path = img_data[\"file_name\"]\n",
        "    img_name = img_path.split(\"/\")[-1]\n",
        "    copyfile(f\"{img_path}\", f\"./lvis/images/valid/{img_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKRsOwDgTZRF"
      },
      "outputs": [],
      "source": [
        "data_yaml = dict(\n",
        "    train=\"../lvis/images/train\", val=\"../lvis/images/valid\", nc=1, names=[\"helmet\"]\n",
        ")\n",
        "\n",
        "with open(\"./yolov5/data/data.yaml\", \"w\") as outfile:\n",
        "    yaml.dump(data_yaml, outfile, default_flow_style=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb_PNjViXMBn"
      },
      "outputs": [],
      "source": [
        "def create_labels(dataset_dicts, is_train=True):\n",
        "    for img_data in tqdm(dataset_dicts):\n",
        "        img_path = img_data[\"file_name\"]\n",
        "        img = cv2.imread(img_path)\n",
        "        img_h, img_w, _ = img.shape\n",
        "\n",
        "        bboxes = []\n",
        "        for i in range(len(img_data[\"bbox\"])):\n",
        "            bbox = img_data[\"bbox\"][i]\n",
        "            x, y, w, h = bbox\n",
        "            # —Ñ–æ—Ä–º–∞—Ç –¥–ª—è YOLO\n",
        "            x_c, y_c, w, h = x + w / 2, y + h / 2, w, h\n",
        "            bboxes.append([x_c / img_w, y_c / img_h, w / img_w, h / img_h])\n",
        "\n",
        "        img_name = img_path.split(\"/\")[-1]\n",
        "        img_name = img_name[:-4]  # remove .jpg\n",
        "\n",
        "        file_name = f'./lvis/labels/{\"train\" if is_train else \"valid\"}/{img_name}.txt'\n",
        "\n",
        "        with open(file_name, \"w\") as f:\n",
        "            for i, bbox in enumerate(bboxes):\n",
        "                label = 0\n",
        "                bbox = [label] + bbox\n",
        "                bbox = [str(i) for i in bbox]\n",
        "                bbox = \" \".join(bbox)\n",
        "                f.write(bbox)\n",
        "                f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDPIOhiwaM8_",
        "outputId": "2fd4c801-1c93-40a1-8239-a8dfa2df67d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1718/1718 [00:10<00:00, 162.88it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 191/191 [00:01<00:00, 164.63it/s]\n"
          ]
        }
      ],
      "source": [
        "create_labels(train_dataset_dicts, is_train=True)\n",
        "create_labels(val_dataset_dicts, is_train=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guXLIebKcLc1",
        "outputId": "ef04473a-c93b-4844-eda2-47bc8174a401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=24, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=lvis, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[12], save_period=1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-41-g10e93d2 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir lvis', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
            "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1     40374  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "Model summary: 445 layers, 86217814 parameters, 86217814 gradients, 204.6 GFLOPs\n",
            "\n",
            "Transferred 739/745 items from yolov5x.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "freezing model.0.conv.weight\n",
            "freezing model.0.bn.weight\n",
            "freezing model.0.bn.bias\n",
            "freezing model.1.conv.weight\n",
            "freezing model.1.bn.weight\n",
            "freezing model.1.bn.bias\n",
            "freezing model.2.cv1.conv.weight\n",
            "freezing model.2.cv1.bn.weight\n",
            "freezing model.2.cv1.bn.bias\n",
            "freezing model.2.cv2.conv.weight\n",
            "freezing model.2.cv2.bn.weight\n",
            "freezing model.2.cv2.bn.bias\n",
            "freezing model.2.cv3.conv.weight\n",
            "freezing model.2.cv3.bn.weight\n",
            "freezing model.2.cv3.bn.bias\n",
            "freezing model.2.m.0.cv1.conv.weight\n",
            "freezing model.2.m.0.cv1.bn.weight\n",
            "freezing model.2.m.0.cv1.bn.bias\n",
            "freezing model.2.m.0.cv2.conv.weight\n",
            "freezing model.2.m.0.cv2.bn.weight\n",
            "freezing model.2.m.0.cv2.bn.bias\n",
            "freezing model.2.m.1.cv1.conv.weight\n",
            "freezing model.2.m.1.cv1.bn.weight\n",
            "freezing model.2.m.1.cv1.bn.bias\n",
            "freezing model.2.m.1.cv2.conv.weight\n",
            "freezing model.2.m.1.cv2.bn.weight\n",
            "freezing model.2.m.1.cv2.bn.bias\n",
            "freezing model.2.m.2.cv1.conv.weight\n",
            "freezing model.2.m.2.cv1.bn.weight\n",
            "freezing model.2.m.2.cv1.bn.bias\n",
            "freezing model.2.m.2.cv2.conv.weight\n",
            "freezing model.2.m.2.cv2.bn.weight\n",
            "freezing model.2.m.2.cv2.bn.bias\n",
            "freezing model.2.m.3.cv1.conv.weight\n",
            "freezing model.2.m.3.cv1.bn.weight\n",
            "freezing model.2.m.3.cv1.bn.bias\n",
            "freezing model.2.m.3.cv2.conv.weight\n",
            "freezing model.2.m.3.cv2.bn.weight\n",
            "freezing model.2.m.3.cv2.bn.bias\n",
            "freezing model.3.conv.weight\n",
            "freezing model.3.bn.weight\n",
            "freezing model.3.bn.bias\n",
            "freezing model.4.cv1.conv.weight\n",
            "freezing model.4.cv1.bn.weight\n",
            "freezing model.4.cv1.bn.bias\n",
            "freezing model.4.cv2.conv.weight\n",
            "freezing model.4.cv2.bn.weight\n",
            "freezing model.4.cv2.bn.bias\n",
            "freezing model.4.cv3.conv.weight\n",
            "freezing model.4.cv3.bn.weight\n",
            "freezing model.4.cv3.bn.bias\n",
            "freezing model.4.m.0.cv1.conv.weight\n",
            "freezing model.4.m.0.cv1.bn.weight\n",
            "freezing model.4.m.0.cv1.bn.bias\n",
            "freezing model.4.m.0.cv2.conv.weight\n",
            "freezing model.4.m.0.cv2.bn.weight\n",
            "freezing model.4.m.0.cv2.bn.bias\n",
            "freezing model.4.m.1.cv1.conv.weight\n",
            "freezing model.4.m.1.cv1.bn.weight\n",
            "freezing model.4.m.1.cv1.bn.bias\n",
            "freezing model.4.m.1.cv2.conv.weight\n",
            "freezing model.4.m.1.cv2.bn.weight\n",
            "freezing model.4.m.1.cv2.bn.bias\n",
            "freezing model.4.m.2.cv1.conv.weight\n",
            "freezing model.4.m.2.cv1.bn.weight\n",
            "freezing model.4.m.2.cv1.bn.bias\n",
            "freezing model.4.m.2.cv2.conv.weight\n",
            "freezing model.4.m.2.cv2.bn.weight\n",
            "freezing model.4.m.2.cv2.bn.bias\n",
            "freezing model.4.m.3.cv1.conv.weight\n",
            "freezing model.4.m.3.cv1.bn.weight\n",
            "freezing model.4.m.3.cv1.bn.bias\n",
            "freezing model.4.m.3.cv2.conv.weight\n",
            "freezing model.4.m.3.cv2.bn.weight\n",
            "freezing model.4.m.3.cv2.bn.bias\n",
            "freezing model.4.m.4.cv1.conv.weight\n",
            "freezing model.4.m.4.cv1.bn.weight\n",
            "freezing model.4.m.4.cv1.bn.bias\n",
            "freezing model.4.m.4.cv2.conv.weight\n",
            "freezing model.4.m.4.cv2.bn.weight\n",
            "freezing model.4.m.4.cv2.bn.bias\n",
            "freezing model.4.m.5.cv1.conv.weight\n",
            "freezing model.4.m.5.cv1.bn.weight\n",
            "freezing model.4.m.5.cv1.bn.bias\n",
            "freezing model.4.m.5.cv2.conv.weight\n",
            "freezing model.4.m.5.cv2.bn.weight\n",
            "freezing model.4.m.5.cv2.bn.bias\n",
            "freezing model.4.m.6.cv1.conv.weight\n",
            "freezing model.4.m.6.cv1.bn.weight\n",
            "freezing model.4.m.6.cv1.bn.bias\n",
            "freezing model.4.m.6.cv2.conv.weight\n",
            "freezing model.4.m.6.cv2.bn.weight\n",
            "freezing model.4.m.6.cv2.bn.bias\n",
            "freezing model.4.m.7.cv1.conv.weight\n",
            "freezing model.4.m.7.cv1.bn.weight\n",
            "freezing model.4.m.7.cv1.bn.bias\n",
            "freezing model.4.m.7.cv2.conv.weight\n",
            "freezing model.4.m.7.cv2.bn.weight\n",
            "freezing model.4.m.7.cv2.bn.bias\n",
            "freezing model.5.conv.weight\n",
            "freezing model.5.bn.weight\n",
            "freezing model.5.bn.bias\n",
            "freezing model.6.cv1.conv.weight\n",
            "freezing model.6.cv1.bn.weight\n",
            "freezing model.6.cv1.bn.bias\n",
            "freezing model.6.cv2.conv.weight\n",
            "freezing model.6.cv2.bn.weight\n",
            "freezing model.6.cv2.bn.bias\n",
            "freezing model.6.cv3.conv.weight\n",
            "freezing model.6.cv3.bn.weight\n",
            "freezing model.6.cv3.bn.bias\n",
            "freezing model.6.m.0.cv1.conv.weight\n",
            "freezing model.6.m.0.cv1.bn.weight\n",
            "freezing model.6.m.0.cv1.bn.bias\n",
            "freezing model.6.m.0.cv2.conv.weight\n",
            "freezing model.6.m.0.cv2.bn.weight\n",
            "freezing model.6.m.0.cv2.bn.bias\n",
            "freezing model.6.m.1.cv1.conv.weight\n",
            "freezing model.6.m.1.cv1.bn.weight\n",
            "freezing model.6.m.1.cv1.bn.bias\n",
            "freezing model.6.m.1.cv2.conv.weight\n",
            "freezing model.6.m.1.cv2.bn.weight\n",
            "freezing model.6.m.1.cv2.bn.bias\n",
            "freezing model.6.m.2.cv1.conv.weight\n",
            "freezing model.6.m.2.cv1.bn.weight\n",
            "freezing model.6.m.2.cv1.bn.bias\n",
            "freezing model.6.m.2.cv2.conv.weight\n",
            "freezing model.6.m.2.cv2.bn.weight\n",
            "freezing model.6.m.2.cv2.bn.bias\n",
            "freezing model.6.m.3.cv1.conv.weight\n",
            "freezing model.6.m.3.cv1.bn.weight\n",
            "freezing model.6.m.3.cv1.bn.bias\n",
            "freezing model.6.m.3.cv2.conv.weight\n",
            "freezing model.6.m.3.cv2.bn.weight\n",
            "freezing model.6.m.3.cv2.bn.bias\n",
            "freezing model.6.m.4.cv1.conv.weight\n",
            "freezing model.6.m.4.cv1.bn.weight\n",
            "freezing model.6.m.4.cv1.bn.bias\n",
            "freezing model.6.m.4.cv2.conv.weight\n",
            "freezing model.6.m.4.cv2.bn.weight\n",
            "freezing model.6.m.4.cv2.bn.bias\n",
            "freezing model.6.m.5.cv1.conv.weight\n",
            "freezing model.6.m.5.cv1.bn.weight\n",
            "freezing model.6.m.5.cv1.bn.bias\n",
            "freezing model.6.m.5.cv2.conv.weight\n",
            "freezing model.6.m.5.cv2.bn.weight\n",
            "freezing model.6.m.5.cv2.bn.bias\n",
            "freezing model.6.m.6.cv1.conv.weight\n",
            "freezing model.6.m.6.cv1.bn.weight\n",
            "freezing model.6.m.6.cv1.bn.bias\n",
            "freezing model.6.m.6.cv2.conv.weight\n",
            "freezing model.6.m.6.cv2.bn.weight\n",
            "freezing model.6.m.6.cv2.bn.bias\n",
            "freezing model.6.m.7.cv1.conv.weight\n",
            "freezing model.6.m.7.cv1.bn.weight\n",
            "freezing model.6.m.7.cv1.bn.bias\n",
            "freezing model.6.m.7.cv2.conv.weight\n",
            "freezing model.6.m.7.cv2.bn.weight\n",
            "freezing model.6.m.7.cv2.bn.bias\n",
            "freezing model.6.m.8.cv1.conv.weight\n",
            "freezing model.6.m.8.cv1.bn.weight\n",
            "freezing model.6.m.8.cv1.bn.bias\n",
            "freezing model.6.m.8.cv2.conv.weight\n",
            "freezing model.6.m.8.cv2.bn.weight\n",
            "freezing model.6.m.8.cv2.bn.bias\n",
            "freezing model.6.m.9.cv1.conv.weight\n",
            "freezing model.6.m.9.cv1.bn.weight\n",
            "freezing model.6.m.9.cv1.bn.bias\n",
            "freezing model.6.m.9.cv2.conv.weight\n",
            "freezing model.6.m.9.cv2.bn.weight\n",
            "freezing model.6.m.9.cv2.bn.bias\n",
            "freezing model.6.m.10.cv1.conv.weight\n",
            "freezing model.6.m.10.cv1.bn.weight\n",
            "freezing model.6.m.10.cv1.bn.bias\n",
            "freezing model.6.m.10.cv2.conv.weight\n",
            "freezing model.6.m.10.cv2.bn.weight\n",
            "freezing model.6.m.10.cv2.bn.bias\n",
            "freezing model.6.m.11.cv1.conv.weight\n",
            "freezing model.6.m.11.cv1.bn.weight\n",
            "freezing model.6.m.11.cv1.bn.bias\n",
            "freezing model.6.m.11.cv2.conv.weight\n",
            "freezing model.6.m.11.cv2.bn.weight\n",
            "freezing model.6.m.11.cv2.bn.bias\n",
            "freezing model.7.conv.weight\n",
            "freezing model.7.bn.weight\n",
            "freezing model.7.bn.bias\n",
            "freezing model.8.cv1.conv.weight\n",
            "freezing model.8.cv1.bn.weight\n",
            "freezing model.8.cv1.bn.bias\n",
            "freezing model.8.cv2.conv.weight\n",
            "freezing model.8.cv2.bn.weight\n",
            "freezing model.8.cv2.bn.bias\n",
            "freezing model.8.cv3.conv.weight\n",
            "freezing model.8.cv3.bn.weight\n",
            "freezing model.8.cv3.bn.bias\n",
            "freezing model.8.m.0.cv1.conv.weight\n",
            "freezing model.8.m.0.cv1.bn.weight\n",
            "freezing model.8.m.0.cv1.bn.bias\n",
            "freezing model.8.m.0.cv2.conv.weight\n",
            "freezing model.8.m.0.cv2.bn.weight\n",
            "freezing model.8.m.0.cv2.bn.bias\n",
            "freezing model.8.m.1.cv1.conv.weight\n",
            "freezing model.8.m.1.cv1.bn.weight\n",
            "freezing model.8.m.1.cv1.bn.bias\n",
            "freezing model.8.m.1.cv2.conv.weight\n",
            "freezing model.8.m.1.cv2.bn.weight\n",
            "freezing model.8.m.1.cv2.bn.bias\n",
            "freezing model.8.m.2.cv1.conv.weight\n",
            "freezing model.8.m.2.cv1.bn.weight\n",
            "freezing model.8.m.2.cv1.bn.bias\n",
            "freezing model.8.m.2.cv2.conv.weight\n",
            "freezing model.8.m.2.cv2.bn.weight\n",
            "freezing model.8.m.2.cv2.bn.bias\n",
            "freezing model.8.m.3.cv1.conv.weight\n",
            "freezing model.8.m.3.cv1.bn.weight\n",
            "freezing model.8.m.3.cv1.bn.bias\n",
            "freezing model.8.m.3.cv2.conv.weight\n",
            "freezing model.8.m.3.cv2.bn.weight\n",
            "freezing model.8.m.3.cv2.bn.bias\n",
            "freezing model.9.cv1.conv.weight\n",
            "freezing model.9.cv1.bn.weight\n",
            "freezing model.9.cv1.bn.bias\n",
            "freezing model.9.cv2.conv.weight\n",
            "freezing model.9.cv2.bn.weight\n",
            "freezing model.9.cv2.bn.bias\n",
            "freezing model.10.conv.weight\n",
            "freezing model.10.bn.weight\n",
            "freezing model.10.bn.bias\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005625000000000001), 126 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/lvis/labels/train.cache... 1718 images, 0 backgrounds, 0 corrupt: 100% 1718/1718 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/lvis/labels/valid.cache... 191 images, 0 backgrounds, 0 corrupt: 100% 191/191 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.53 anchors/target, 0.970 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING ‚ö†Ô∏è Extremely small objects found: 105 of 4247 labels are <3 pixels in size\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 4239 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8053: 100% 1000/1000 [00:03<00:00, 288.27it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9979 best possible recall, 5.55 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.381/0.804-mean/best, past_thr=0.533-mean: 6,5, 12,11, 20,17, 28,26, 39,35, 49,48, 65,62, 96,89, 238,170\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ‚úÖ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to lvis/exp5/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mlvis/exp5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/19      6.58G     0.1005    0.03103          0         38        640: 100% 72/72 [01:18<00:00,  1.10s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.50s/it]\n",
            "                   all        191        605      0.371      0.387      0.272     0.0864\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/19      11.5G    0.07263    0.02524          0         88        640: 100% 72/72 [01:09<00:00,  1.03it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.31s/it]\n",
            "                   all        191        605      0.415      0.426      0.387      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/19      11.5G    0.06367    0.02164          0         31        640: 100% 72/72 [01:08<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.31s/it]\n",
            "                   all        191        605      0.608      0.532       0.57      0.219\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/19      11.5G    0.05575    0.02029          0         67        640: 100% 72/72 [01:09<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.31s/it]\n",
            "                   all        191        605      0.753      0.635      0.707       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/19      11.5G    0.05088    0.01852          0         38        640: 100% 72/72 [01:10<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.26s/it]\n",
            "                   all        191        605      0.794      0.638      0.714      0.335\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/19      11.5G     0.0465    0.01765          0         36        640: 100% 72/72 [01:09<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.26s/it]\n",
            "                   all        191        605      0.789      0.684      0.746      0.398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/19      11.5G    0.04344    0.01784          0         45        640: 100% 72/72 [01:09<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.29s/it]\n",
            "                   all        191        605      0.862      0.664      0.763      0.399\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/19      11.5G    0.04117    0.01658          0         67        640: 100% 72/72 [01:10<00:00,  1.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.38s/it]\n",
            "                   all        191        605      0.813      0.694      0.777      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/19      11.5G    0.03983    0.01672          0         56        640: 100% 72/72 [01:09<00:00,  1.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:05<00:00,  1.38s/it]\n",
            "                   all        191        605      0.851      0.699      0.775      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/19      11.5G    0.03763    0.01534          0         35        640: 100% 72/72 [01:08<00:00,  1.05it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:04<00:00,  1.23s/it]\n",
            "                   all        191        605      0.855      0.674       0.78      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/19      11.5G    0.03699    0.01535          0        105        640:  22% 16/72 [00:14<00:52,  1.07it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"yolov5/train.py\", line 634, in <module>\n",
            "    main(opt)\n",
            "  File \"yolov5/train.py\", line 528, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"yolov5/train.py\", line 310, in train\n",
            "    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python yolov5/train.py --batch 24 \\\n",
        "                        --epochs 10 \\\n",
        "                        --data data.yaml \\\n",
        "                        --weights yolov5x.pt \\\n",
        "                        --freeze 12 \\\n",
        "                        --save-period 1 \\\n",
        "                        --project lvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JtXPhCwmWVv",
        "outputId": "f9cec55b-9dd5-43f5-cf55-c29f11608350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "000000503707.jpg\n"
          ]
        }
      ],
      "source": [
        "idx = np.random.randint(0, len(val_dataset_dicts))\n",
        "file_path = val_dataset_dicts[idx][\"file_name\"]\n",
        "file_name = file_path.split(\"/\")[-1]\n",
        "print(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvNOiD-WcW8a",
        "outputId": "5de3225d-720d-448f-9b2f-a02baa3361fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./lvis/exp5/weights/last.pt'], source=./lvis/images/valid/000000503707.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=lvis, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-41-g10e93d2 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 322 layers, 86173414 parameters, 0 gradients, 203.8 GFLOPs\n",
            "image 1/1 /content/lvis/images/valid/000000503707.jpg: 448x640 3 helmets, 72.5ms\n",
            "Speed: 0.7ms pre-process, 72.5ms inference, 1.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mlvis/exp10\u001b[0m\n",
            "1 labels saved to lvis/exp10/labels\n"
          ]
        }
      ],
      "source": [
        "!python yolov5/detect.py --weights ./lvis/exp5/weights/last.pt \\\n",
        "                         --source ./lvis/images/valid/{file_name} \\\n",
        "                         --save-txt \\\n",
        "                         --save-conf \\\n",
        "                         --project lvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw4LEpSA-6bO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "57fdf6ced3497be6751aff0d610660c5baf34a48c3eaed9bf5963df6523fe9d6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
